{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivyyang94134/GPT-SoVITS/blob/main/FB%E7%A4%BE%E7%BE%A4%E7%88%AC%E8%9F%B2%E6%95%99%E5%AD%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FB 貼文爬蟲"
      ],
      "metadata": {
        "id": "RhN2N2S9s1NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##下載 寶可夢多張圖片"
      ],
      "metadata": {
        "id": "tLH5b6z4diqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MJ6c9_H9djLz",
        "outputId": "e8ca4807-3d49-459b-c3f3-445ba7c2e388"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio)\n",
            "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.0 (from gradio)\n",
            "  Downloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=117dfb3906be0cacb091ef9e84e589180f96bc87f0a6f1256dc85ce91ee2c9bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, gradio-client, fastapi-cli, altair, fastapi, gradio\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed aiofiles-23.2.1 altair-5.3.0 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.38.1 gradio-client-1.1.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.5.2 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import concurrent.futures\n",
        "import gradio as gr\n",
        "\n",
        "def download_pokemon_image(pokemon_id):\n",
        "    url = f'https://tw.portal-pokemon.com/play/pokedex/{pokemon_id:04d}'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    img = soup.select('meta[property=\"og:image\"]')\n",
        "    if img:\n",
        "        img_url = img[0]['content']\n",
        "        img_response = requests.get(img_url)\n",
        "        if img_response.status_code == 200:\n",
        "            filename = f'pokemon/{pokemon_id:04d}.png'\n",
        "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(img_response.content)\n",
        "            return filename\n",
        "    return None\n",
        "\n",
        "def download_multiple_pokemon(start_id, end_id):\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_id = {executor.submit(download_pokemon_image, i): i for i in range(start_id, end_id+1)}\n",
        "        downloaded_files = []\n",
        "        for future in concurrent.futures.as_completed(future_to_id):\n",
        "            filename = future.result()\n",
        "            if filename:\n",
        "                downloaded_files.append(filename)\n",
        "    return downloaded_files\n",
        "\n",
        "def download_and_display(start_id, end_id):\n",
        "    files = download_multiple_pokemon(start_id, end_id)\n",
        "    return files\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=download_and_display,\n",
        "    inputs=[\n",
        "        gr.Number(label=\"起始ID\", minimum=1, maximum=1000, step=1),\n",
        "        gr.Number(label=\"結束ID\", minimum=1, maximum=1000, step=1)\n",
        "    ],\n",
        "    outputs=gr.Gallery(label=\"下載的寶可夢圖片\"),\n",
        "    title=\"寶可夢圖片下載器\",\n",
        "    description=\"輸入起始和結束的寶可夢ID,下載並顯示對應的寶可夢圖片。\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "collapsed": true,
        "id": "K4jNZ-Judllt",
        "outputId": "1a990967-870f-4c8f-cdc0-af5265de8101"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4cc4be8dc821ad8e4e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4cc4be8dc821ad8e4e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PTT扒光正妹圖片\n",
        "~~~\n",
        "https://www.ptt.cc/bbs/Beauty/index.html"
      ],
      "metadata": {
        "id": "szTcGPS6d6Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nf7yk6MjeCa1",
        "outputId": "0356e1ed-7cb2-4203-cf1e-29ac87549070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (5.3.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from google.colab import output\n",
        "import urllib3\n",
        "import json\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "def fetch_ptt_images(url):\n",
        "    try:\n",
        "        print(f\"Attempting to fetch images from URL: {url}\")\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        web = requests.get(url, cookies={'over18':'1'}, headers=headers, verify=False, allow_redirects=False)\n",
        "\n",
        "        print(f\"Request status code: {web.status_code}\")\n",
        "        print(f\"Response headers: {json.dumps(dict(web.headers), indent=2)}\")\n",
        "        print(f\"Response content (first 500 characters): {web.text[:500]}\")\n",
        "\n",
        "        if web.status_code == 302:\n",
        "            print(f\"Redirect detected. Location: {web.headers.get('Location')}\")\n",
        "            return []\n",
        "\n",
        "        web.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(web.text, \"html.parser\")\n",
        "        main_content = soup.find('div', id='main-content')\n",
        "        if main_content:\n",
        "            imgs = main_content.find_all('img')\n",
        "            print(\"Found main-content div\")\n",
        "        else:\n",
        "            imgs = soup.find_all('img')\n",
        "            print(\"Main-content div not found, searching in entire page\")\n",
        "\n",
        "        image_urls = [img['src'] for img in imgs]\n",
        "        print(f\"Found {len(image_urls)} images\")\n",
        "        print(f\"Image URLs: {image_urls}\")\n",
        "        return image_urls\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching images: {e}\")\n",
        "        return []\n",
        "\n",
        "def display_images(url):\n",
        "    image_urls = fetch_ptt_images(url)\n",
        "    if not image_urls:\n",
        "        print(\"No images found or error occurred\")\n",
        "        return []\n",
        "    return image_urls  # 直接返回圖片 URL，而不是下載圖片\n",
        "\n",
        "def on_select(evt: gr.SelectData, images):\n",
        "    return images[evt.index]\n",
        "\n",
        "def fetch_and_display(url):\n",
        "    images = display_images(url)\n",
        "    error = \"\" if images else \"無法獲取圖片,請檢查 URL 是否正確。詳細錯誤信息請查看控制台輸出。\"\n",
        "    return images, error\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# PTT 正妹下載器\")\n",
        "    with gr.Row():\n",
        "        url_input = gr.Textbox(label=\"輸入 PTT 網頁 URL\")\n",
        "        fetch_button = gr.Button(\"獲取圖片\")\n",
        "\n",
        "    gallery = gr.Gallery(label=\"圖片預覽\", columns=4)\n",
        "    error_output = gr.Textbox(label=\"錯誤信息\")\n",
        "\n",
        "    fetch_button.click(fetch_and_display, inputs=url_input, outputs=[gallery, error_output])\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "wfWm8XM7eLdO",
        "outputId": "e7a4ef80-4aa4-4e38-e353-3e87a96fe18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://566d2285833f5bc8ee.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://566d2285833f5bc8ee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to fetch images from URL: https://www.ptt.cc/bbs/Beauty/M.1721122504.A.E63.html\n",
            "Request status code: 200\n",
            "Response headers: {\n",
            "  \"Date\": \"Tue, 16 Jul 2024 12:16:35 GMT\",\n",
            "  \"Content-Type\": \"text/html; charset=utf-8\",\n",
            "  \"Transfer-Encoding\": \"chunked\",\n",
            "  \"Connection\": \"keep-alive\",\n",
            "  \"strict-transport-security\": \"max-age=0\",\n",
            "  \"CF-Cache-Status\": \"DYNAMIC\",\n",
            "  \"Report-To\": \"{\\\"endpoints\\\":[{\\\"url\\\":\\\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v4?s=NT9IeKkldxTRmBzIOz%2Bz7mV1WbTiYz%2BpwsJPVl77c3WA2RHpDU3qGIrBdcUcpsLjm8pMjpEkYWOjNqotHxLykac9lfZk9CH797pe8%2FLpJ2WYOORNElsnSYD73NKX\\\"}],\\\"group\\\":\\\"cf-nel\\\",\\\"max_age\\\":604800}\",\n",
            "  \"NEL\": \"{\\\"success_fraction\\\":0,\\\"report_to\\\":\\\"cf-nel\\\",\\\"max_age\\\":604800}\",\n",
            "  \"X-Content-Type-Options\": \"nosniff\",\n",
            "  \"Server\": \"cloudflare\",\n",
            "  \"CF-RAY\": \"8a41ec9e0ecdab97-SJC-PIG\",\n",
            "  \"Content-Encoding\": \"gzip\",\n",
            "  \"alt-svc\": \"h3=\\\":443\\\"; ma=86400\"\n",
            "}\n",
            "Response content (first 500 characters): <!DOCTYPE html>\n",
            "<html>\n",
            "\t<head>\n",
            "\t\t<meta charset=\"utf-8\">\n",
            "\t\t\n",
            "\n",
            "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "\n",
            "<title>[正妹] 齋藤愛莉 - 看板 Beauty - 批踢踢實業坊</title>\n",
            "<meta name=\"robots\" content=\"all\">\n",
            "<meta name=\"keywords\" content=\"Ptt BBS 批踢踢\">\n",
            "<meta name=\"description\" content=\"https://i.imgur.com/bSZDv9M.jpeg\n",
            "https://i.imgur.com/zrHr9zU.jpeg\n",
            "https://i.imgur.com/T4COBcA.jpeg\n",
            "https://i.imgur.com/eZj0s38.jpeg\n",
            "https://i.imgur.com/1Yss0mq.jpeg\n",
            "\">\n",
            "<meta property=\"og:site_name\" content=\"Ptt\n",
            "Found main-content div\n",
            "Found 8 images\n",
            "Image URLs: ['https://cache.ptt.cc/c/https/i.imgur.com/bSZDv9Ml.jpeg?e=1721300008&s=ZZrxOGy8ucdQxSmfA7fPAw', 'https://cache.ptt.cc/c/https/i.imgur.com/zrHr9zUl.jpeg?e=1721297573&s=12xQSR4DiC3NBTaK_a-AXg', 'https://cache.ptt.cc/c/https/i.imgur.com/T4COBcAl.jpeg?e=1721280741&s=O79yG-MSIDaOKtrjQ0ia-Q', 'https://cache.ptt.cc/c/https/i.imgur.com/eZj0s38l.jpeg?e=1721283990&s=_cAdNLO6O4Znv82WwbEYXA', 'https://cache.ptt.cc/c/https/i.imgur.com/1Yss0mql.jpeg?e=1721238307&s=cpCzAqOjxw4Vy8sjaYDPpQ', 'https://cache.ptt.cc/c/https/i.imgur.com/jSb9CEEl.jpeg?e=1721250622&s=7RH1HxJPcDlh0EgaA_XjgA', 'https://cache.ptt.cc/c/https/i.imgur.com/oA52SUtl.jpeg?e=1721250246&s=Ih8a_QIMxIdirBg3o3N4VA', 'https://cache.ptt.cc/c/https/i.imgur.com/qMkZK3xl.jpeg?e=1721284780&s=kTkyzzedBu9RhXSFpu2JrA']\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://04ce1914c88f47722d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://566d2285833f5bc8ee.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##以GAS 做爬蟲\n",
        "~~~\n",
        "// 主函數：爬取 Facebook 頁面\n",
        "function scrapeFacebookPage() {\n",
        "  var ui = SpreadsheetApp.getUi();\n",
        "  \n",
        "  // 提示用戶手動登入\n",
        "  var loginResult = ui.alert(\n",
        "    '手動登入提示',\n",
        "    '請在瀏覽器中手動登入 Facebook，然後點擊\"確定\"繼續。',\n",
        "    ui.ButtonSet.OK_CANCEL\n",
        "  );\n",
        "  \n",
        "  if (loginResult != ui.Button.OK) {\n",
        "    Logger.log('用戶取消了操作');\n",
        "    return;\n",
        "  }\n",
        "  \n",
        "  // 獲取用戶的 cookies\n",
        "  var cookies = getCookiesFromUser();\n",
        "  if (!cookies) {\n",
        "    Logger.log('未能獲取有效的 cookies，無法繼續爬取');\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  // 設置請求選項\n",
        "  var options = {\n",
        "    'method': 'get',\n",
        "    'headers': {\n",
        "      'Cookie': cookies\n",
        "    },\n",
        "    'muteHttpExceptions': true // 不拋出 HTTP 錯誤，以便我們可以自行處理\n",
        "  };\n",
        "\n",
        "  // 準備 Google 試算表\n",
        "  var sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet();\n",
        "  sheet.clear(); // 清空試算表\n",
        "  // 添加欄位標題\n",
        "  sheet.appendRow(['發布日期', '發布時間', '作者', '貼文內容', '讚數', '留言數', '分享數', '貼文連結']);\n",
        "\n",
        "  var postsScraped = 0; // 已爬取的貼文數量\n",
        "  var nextPageUrl = CONFIG.targetPage; // 初始頁面 URL\n",
        "\n",
        "  // 主要爬取循環\n",
        "  while (postsScraped < CONFIG.maxPosts && nextPageUrl) {\n",
        "    // 添加延遲以避免被封鎖\n",
        "    Utilities.sleep(CONFIG.delayBetweenRequests);\n",
        "\n",
        "    try {\n",
        "      // 獲取頁面內容\n",
        "      var pageResponse = UrlFetchApp.fetch(nextPageUrl, options);\n",
        "      var pageContent = pageResponse.getContentText();\n",
        "\n",
        "      Logger.log('頁面響應狀態: ' + pageResponse.getResponseCode());\n",
        "      Logger.log('頁面內容片段: ' + pageContent.substring(0, 500) + '...'); // 記錄頁面內容的前500個字符\n",
        "\n",
        "      // 提取貼文\n",
        "      var posts = extractPosts(pageContent);\n",
        "\n",
        "      if (posts.length > 0) {\n",
        "        Logger.log('找到 ' + posts.length + ' 條貼文');\n",
        "        for (var i = 0; i < posts.length && postsScraped < CONFIG.maxPosts; i++) {\n",
        "          var post = posts[i];\n",
        "          // 將提取的數據添加到試算表中\n",
        "          sheet.appendRow([\n",
        "            post.date, post.time, post.author, post.content,\n",
        "            post.likes, post.comments, post.shares, post.link\n",
        "          ]);\n",
        "          postsScraped++;\n",
        "        }\n",
        "      } else {\n",
        "        Logger.log('在頁面中未找到貼文');\n",
        "        Logger.log('頁面標題: ' + extractPageTitle(pageContent));\n",
        "      }\n",
        "\n",
        "      // 獲取下一頁的 URL\n",
        "      nextPageUrl = extractNextPageUrl(pageContent);\n",
        "    } catch (error) {\n",
        "      Logger.log('訪問頁面時發生錯誤: ' + error.message);\n",
        "      throw error; // 重新拋出錯誤以停止執行\n",
        "    }\n",
        "  }\n",
        "\n",
        "  Logger.log('爬蟲完成，共爬取了 ' + postsScraped + ' 條貼文。');\n",
        "}\n",
        "\n",
        "// 從用戶獲取 cookies\n",
        "function getCookiesFromUser() {\n",
        "  var ui = SpreadsheetApp.getUi();\n",
        "  var result = ui.prompt(\n",
        "    '輸入 Facebook Cookies',\n",
        "    '請從瀏覽器中複製 Facebook 的 cookies 並粘貼在這裡：',\n",
        "    ui.ButtonSet.OK_CANCEL\n",
        "  );\n",
        "\n",
        "  if (result.getSelectedButton() == ui.Button.OK) {\n",
        "    return result.getResponseText();\n",
        "  } else {\n",
        "    return null;\n",
        "  }\n",
        "}\n",
        "\n",
        "// 提取貼文（更靈活的方法）\n",
        "function extractPosts(pageContent) {\n",
        "  var posts = [];\n",
        "  var postRegex = /<div[^>]*?role=\"article\"[^>]*>([\\s\\S]*?)<\\/div>\\s*<div[^>]*?role=\"article\"/g;\n",
        "  var match;\n",
        "\n",
        "  while ((match = postRegex.exec(pageContent)) !== null) {\n",
        "    var postContent = match[1];\n",
        "    posts.push({\n",
        "      date: extractDate(postContent),\n",
        "      time: extractTime(postContent),\n",
        "      author: extractAuthor(postContent),\n",
        "      content: extractContent(postContent),\n",
        "      likes: extractReactions(postContent, '讚'),\n",
        "      comments: extractReactions(postContent, '留言'),\n",
        "      shares: extractReactions(postContent, '分享'),\n",
        "      link: extractPostLink(postContent)\n",
        "    });\n",
        "  }\n",
        "\n",
        "  return posts;\n",
        "}\n",
        "\n",
        "// 提取日期\n",
        "function extractDate(postContent) {\n",
        "  var match = postContent.match(/(\\d{1,2})月(\\d{1,2})日/);\n",
        "  return match ? match[0] : '未知日期';\n",
        "}\n",
        "\n",
        "// 提取時間\n",
        "function extractTime(postContent) {\n",
        "  var match = postContent.match(/(\\d{1,2}):(\\d{2})/);\n",
        "  return match ? match[0] : '未知時間';\n",
        "}\n",
        "\n",
        "// 提取作者\n",
        "function extractAuthor(postContent) {\n",
        "  var match = postContent.match(/<h3[^>]*><a[^>]*>(.*?)<\\/a>/);\n",
        "  return match ? match[1] : '未知作者';\n",
        "}\n",
        "\n",
        "// 提取貼文內容\n",
        "function extractContent(postContent) {\n",
        "  var match = postContent.match(/<p>([\\s\\S]*?)<\\/p>/);\n",
        "  return match ? match[1].trim() : '無內容';\n",
        "}\n",
        "\n",
        "// 提取反應數（讚、留言、分享）\n",
        "function extractReactions(postContent, type) {\n",
        "  var match = postContent.match(new RegExp(type + '\\\\s*(\\\\d+)'));\n",
        "  return match ? match[1] : '0';\n",
        "}\n",
        "\n",
        "// 提取貼文連結\n",
        "function extractPostLink(postContent) {\n",
        "  var match = postContent.match(/href=\"(\\/(?:groups|[^\\/]+)\\/[^\"]*?)\"/);\n",
        "  return match ? 'https://mbasic.facebook.com' + match[1] : '無連結';\n",
        "}\n",
        "\n",
        "// 提取下一頁 URL\n",
        "function extractNextPageUrl(pageContent) {\n",
        "  var match = pageContent.match(/href=\"([^\"]*?)\">顯示更多<\\/a>/);\n",
        "  return match ? 'https://mbasic.facebook.com' + match[1] : null;\n",
        "}\n",
        "\n",
        "// 提取頁面標題\n",
        "function extractPageTitle(pageContent) {\n",
        "  var match = pageContent.match(/<title>(.*?)<\\/title>/);\n",
        "  return match ? match[1] : '無標題';\n",
        "}\n",
        "\n",
        "// 創建一個選單項來運行爬蟲\n",
        "function onOpen() {\n",
        "  var ui = SpreadsheetApp.getUi();\n",
        "  ui.createMenu('Facebook 爬蟲')\n",
        "    .addItem('爬取 Facebook 頁面', 'scrapeFacebookPage')\n",
        "    .addItem('更改爬取目標', 'changeTargetPage')\n",
        "    .addToUi();\n",
        "}\n",
        "\n",
        "// 用於更改爬取目標的函數\n",
        "function changeTargetPage() {\n",
        "  var ui = SpreadsheetApp.getUi();\n",
        "  var result = ui.prompt(\n",
        "    '更改爬取目標',\n",
        "    '請輸入要爬取的 Facebook 頁面 mbasic URL：',\n",
        "    ui.ButtonSet.OK_CANCEL);\n",
        "\n",
        "  var button = result.getSelectedButton();\n",
        "  var text = result.getResponseText();\n",
        "  if (button == ui.Button.OK) {\n",
        "    CONFIG.targetPage = text;\n",
        "    ui.alert('爬取目標已更改為：' + text);\n",
        "  } else if (button == ui.Button.CANCEL) {\n",
        "    ui.alert('已取消更改。');\n",
        "  } else if (button == ui.Button.CLOSE) {\n",
        "    ui.alert('您關閉了對話框。');\n",
        "  }\n",
        "}\n",
        "\n",
        "// 注意事項：\n",
        "// 1. 此腳本需要用戶手動登入 Facebook 並提供 cookies，以避免自動登入可能帶來的帳戶風險。\n",
        "// 2. 使用爬蟲可能違反 Facebook 的服務條款。請確保您有權限訪問和使用您爬取的數據。\n",
        "// 3. Facebook 可能會改變其網頁結構，導致爬蟲失效。您可能需要定期更新正則表達式以匹配新的 HTML 結構。\n",
        "// 4. 本腳本僅供教育和研究目的使用。在實際應用中，請遵守所有相關法律和平台政策。\n",
        "\n",
        "~~~"
      ],
      "metadata": {
        "id": "8eI1CxDSWQMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "quzOxMHQSQB3",
        "outputId": "972ff066-6e43-49a5-b724-a31b76fd19cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 導入必要的庫\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 設置配置信息\n",
        "CONFIG = {\n",
        "    'targetPage': 'https://mbasic.facebook.com/groups/3208727879422567/',  # 目標 Facebook 頁面的 mbasic URL\n",
        "    'maxPosts': 50,  # 最多爬取的貼文數量\n",
        "    'delayBetweenRequests': 5  # 每次請求之間的延遲（秒）\n",
        "}\n",
        "\n",
        "def get_cookies_from_user():\n",
        "    \"\"\"\n",
        "    從用戶獲取 Facebook cookies\n",
        "    返回：包含 cookies 的字典\n",
        "    \"\"\"\n",
        "    cookies_string = input(\"請從瀏覽器中複製 Facebook 的 cookies 並粘貼在這裡：\")\n",
        "    cookies = {}\n",
        "    for cookie in cookies_string.split(';'):\n",
        "        key, value = cookie.strip().split('=', 1)\n",
        "        cookies[key] = value\n",
        "    return cookies\n",
        "\n",
        "def scrape_facebook_page(cookies):\n",
        "    \"\"\"\n",
        "    爬取 Facebook 頁面\n",
        "    參數：cookies - 包含用戶 cookies 的字典\n",
        "    返回：爬取到的貼文列表\n",
        "    \"\"\"\n",
        "    posts = []\n",
        "    next_page_url = CONFIG['targetPage']\n",
        "\n",
        "    while len(posts) < CONFIG['maxPosts'] and next_page_url:\n",
        "        print(f\"正在爬取頁面：{next_page_url}\")\n",
        "        response = requests.get(next_page_url, cookies=cookies)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 提取當前頁面的所有貼文\n",
        "        articles = soup.find_all('div', role='article')\n",
        "        for article in articles:\n",
        "            if len(posts) >= CONFIG['maxPosts']:\n",
        "                break\n",
        "            post = extract_post_info(article)\n",
        "            posts.append(post)\n",
        "\n",
        "        # 尋找下一頁的連結\n",
        "        next_page_link = soup.find('a', string='顯示更多')\n",
        "        next_page_url = 'https://mbasic.facebook.com' + next_page_link['href'] if next_page_link else None\n",
        "\n",
        "        # 延遲以避免被封鎖\n",
        "        time.sleep(CONFIG['delayBetweenRequests'])\n",
        "\n",
        "    return posts\n",
        "\n",
        "def extract_post_info(article):\n",
        "    \"\"\"\n",
        "    從 HTML 中提取單個貼文的信息\n",
        "    參數：article - 包含貼文信息的 BeautifulSoup 對象\n",
        "    返回：包含貼文信息的字典\n",
        "    \"\"\"\n",
        "    # 提取作者\n",
        "    author_tag = article.find('h3')\n",
        "    author = author_tag.text if author_tag else \"未知作者\"\n",
        "\n",
        "    # 提取發布時間\n",
        "    time_tag = article.find('abbr')\n",
        "    post_time = time_tag.text if time_tag else \"未知時間\"\n",
        "\n",
        "    # 提取貼文內容\n",
        "    content_tag = article.find('p')\n",
        "    content = content_tag.text if content_tag else \"無內容\"\n",
        "\n",
        "    # 提取互動數據（讚、留言、分享）\n",
        "    footer = article.find('footer')\n",
        "    likes = comments = shares = 0\n",
        "    if footer:\n",
        "        likes_match = re.search(r'(\\d+)\\s*讚', footer.text)\n",
        "        comments_match = re.search(r'(\\d+)\\s*則留言', footer.text)\n",
        "        shares_match = re.search(r'(\\d+)\\s*次分享', footer.text)\n",
        "        likes = int(likes_match.group(1)) if likes_match else 0\n",
        "        comments = int(comments_match.group(1)) if comments_match else 0\n",
        "        shares = int(shares_match.group(1)) if shares_match else 0\n",
        "\n",
        "    # 提取貼文連結\n",
        "    link_tag = article.find('a', string='完整動態')\n",
        "    link = 'https://mbasic.facebook.com' + link_tag['href'] if link_tag else \"無連結\"\n",
        "\n",
        "    return {\n",
        "        '作者': author,\n",
        "        '發布時間': post_time,\n",
        "        '內容': content,\n",
        "        '讚數': likes,\n",
        "        '留言數': comments,\n",
        "        '分享數': shares,\n",
        "        '連結': link\n",
        "    }\n",
        "\n",
        "# 主程序\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Facebook 爬蟲程式啟動\")\n",
        "    print(\"請確保您已經手動登入 Facebook\")\n",
        "    cookies = get_cookies_from_user()\n",
        "    posts = scrape_facebook_page(cookies)\n",
        "\n",
        "    # 將爬取的數據轉換為 DataFrame\n",
        "    df = pd.DataFrame(posts)\n",
        "\n",
        "    # 顯示爬取結果的摘要\n",
        "    print(f\"\\n共爬取了 {len(posts)} 條貼文\")\n",
        "    print(df.head())\n",
        "\n",
        "    # 將結果保存為 CSV 檔案\n",
        "    df.to_csv('facebook_posts.csv', index=False)\n",
        "    print(\"\\n爬取的數據已保存到 facebook_posts.csv 檔案中\")\n",
        "\n",
        "# 注意事項：\n",
        "# 1. 此腳本需要用戶手動登入 Facebook 並提供 cookies，以避免自動登入可能帶來的帳戶風險。\n",
        "# 2. 使用爬蟲可能違反 Facebook 的服務條款。請確保您有權限訪問和使用您爬取的數據。\n",
        "# 3. Facebook 可能會改變其網頁結構，導致爬蟲失效。您可能需要定期更新提取邏輯以匹配新的 HTML 結構。\n",
        "# 4. 本腳本僅供教育和研究目的使用。在實際應用中，請遵守所有相關法律和平台政策。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "SNWdZiKUSS_C",
        "outputId": "25c4b2ab-32c5-42c0-82bd-56ba917179dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Facebook 爬蟲程式啟動\n",
            "請確保您已經手動登入 Facebook\n",
            "請從瀏覽器中複製 Facebook 的 cookies 並粘貼在這裡：# Netscape HTTP Cookie File # http://curl.haxx.se/rfc/cookie_spec.html # This is a generated file!  Do not edit.  .facebook.com\tTRUE\t/\tTRUE\t1755685710\tsb\t1OIaZcN75qrjAKYRc3tyV3jJ .facebook.com\tTRUE\t/\tTRUE\t1730820821\tdatr\t1OIaZQPOHb4jOL3ocPAvtft8 .facebook.com\tTRUE\t/\tTRUE\t1721735335\tdpr\t1.125 .facebook.com\tTRUE\t/\tTRUE\t1755685692\tps_n\t1 .facebook.com\tTRUE\t/\tTRUE\t1755685692\tps_l\t1 .facebook.com\tTRUE\t/\tTRUE\t1752661710\tc_user\t61561847643757 .facebook.com\tTRUE\t/\tTRUE\t1752661710\txs\t38%3AE1RyxK2WSxmtKg%3A2%3A1721125711%3A-1%3A-1 .facebook.com\tTRUE\t/\tTRUE\t1728901710\tfr\t1DVhp2YqWYkRjoeC0.AWUjeEJvhs6HRZTGwhvpYGm96o8.BmlksL..AAA.0.0.BmlktO.AWX5x2wpq5A .facebook.com\tTRUE\t/\tTRUE\t1721735335\twd\t904x772 .facebook.com\tTRUE\t/\tTRUE\t0\tpresence\tC%7B%22t3%22%3A%5B%5D%2C%22utc3%22%3A1721130534003%2C%22v%22%3A1%7D .www.facebook.com\tTRUE\t/\tTRUE\t1755690535\tm_ls\t%7B%2261561847643757%22%3A%7B%22c%22%3A%7B%221%22%3A%22HCwAABYQFoKF36gFEwUW2vn7wK__GwA%22%2C%222%22%3A%22GRwVQBxMAAAWARaorbLpDBYAFqitsukMABYoAA%22%2C%2295%22%3A%22HCwAABYGFta87uwEEwUW2vn7wK__GwA%22%7D%2C%22d%22%3A%22539dcc10-5b5a-4c2c-b64d-eeefa9861729%22%2C%22s%22%3A%221%22%2C%22u%22%3A%2255mn1q%22%7D%7D\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-49afea0bb78b>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Facebook 爬蟲程式啟動\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"請確保您已經手動登入 Facebook\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mcookies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cookies_from_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_facebook_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-49afea0bb78b>\u001b[0m in \u001b[0;36mget_cookies_from_user\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcookies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcookie\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcookies_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcookie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mcookies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcookies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##進化套件 Facebook-Scraper"
      ],
      "metadata": {
        "id": "KuSSSDbycSOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 安裝必要的套件\n",
        "!pip install facebook-scraper pandas tqdm"
      ],
      "metadata": {
        "id": "zGLo6rfDcVgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 導入所需的庫\n",
        "import pandas as pd\n",
        "from facebook_scraper import get_posts\n",
        "from datetime import datetime\n",
        "from time import sleep\n",
        "from random import randint\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# 設置配置\n",
        "CONFIG = {\n",
        "    'fanpage': 'taipei33',  # 目標粉絲頁的 ID\n",
        "    'pages': 200,  # 預設爬取頁數\n",
        "    'year_limit': 2019,  # 設置年份限制，早於此年份的貼文不爬取\n",
        "    'sleep_min': 10,  # 每次爬取後的最小睡眠時間（秒）\n",
        "    'sleep_max': 60,  # 每次爬取後的最大睡眠時間（秒）\n",
        "}\n",
        "\n",
        "def get_cookies():\n",
        "    \"\"\"從使用者獲取 cookies\"\"\"\n",
        "    print(\"請提供 Facebook cookies:\")\n",
        "    print(\"1. 登入 Facebook\")\n",
        "    print(\"2. 打開瀏覽器開發者工具（F12）\")\n",
        "    print(\"3. 在 Network 標籤中找到對 facebook.com 的請求\")\n",
        "    print(\"4. 在 Headers 中複製 Cookie 的值\")\n",
        "    return input(\"請將 cookies 粘貼在這裡: \").strip()\n",
        "\n",
        "def scrape_facebook(cookies):\n",
        "    \"\"\"主要的爬蟲函數\"\"\"\n",
        "    P = pd.DataFrame()\n",
        "    i = 0\n",
        "\n",
        "    print(f\"開始爬取 {CONFIG['fanpage']} 的貼文...\")\n",
        "\n",
        "    try:\n",
        "        for post in tqdm(get_posts(CONFIG['fanpage'], pages=CONFIG['pages'], cookies=cookies, options={\"reactors\": True}), desc=\"爬取進度\"):\n",
        "            if int(post['time'].strftime(\"%Y\")) <= CONFIG['year_limit']:\n",
        "                print(f\"已達到年份限制 {CONFIG['year_limit']}，停止爬取\")\n",
        "                break\n",
        "\n",
        "            P = P.append({\n",
        "                'user_id': str(post.get('user_id', '')),\n",
        "                'username': str(post.get('username', '')),\n",
        "                'time': post['time'],\n",
        "                'post_url': post['post_url'],\n",
        "                'post_id': str(post['post_id']),\n",
        "                'post_text': post.get('post_text', '').strip().replace(\"\\n\", \" \"),\n",
        "                'like_count': post['reactions'].get('讚', 0) if post.get('reactions') else 0,\n",
        "                'love_count': post['reactions'].get('大心', 0) if post.get('reactions') else 0,\n",
        "                'go_count': post['reactions'].get('加油', 0) if post.get('reactions') else 0,\n",
        "                'wow_count': post['reactions'].get('哇', 0) if post.get('reactions') else 0,\n",
        "                'haha_count': post['reactions'].get('哈', 0) if post.get('reactions') else 0,\n",
        "                'sad_count': post['reactions'].get('嗚', 0) if post.get('reactions') else 0,\n",
        "                'angry_count': post['reactions'].get('怒', 0) if post.get('reactions') else 0,\n",
        "                'share_count': post.get('shares', 0),\n",
        "                'comment_count': post.get('comments', 0),\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            i += 1\n",
        "            print(f\"\\n爬取完成 {i}. POST_ID: {post['post_id']} {post['time']}\\nURL: {post['post_url']}\\n\")\n",
        "\n",
        "            sleep_time = randint(CONFIG['sleep_min'], CONFIG['sleep_max'])\n",
        "            print(f\"休息 {sleep_time} 秒...\")\n",
        "            sleep(sleep_time)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"爬取過程中發生錯誤: {str(e)}\")\n",
        "\n",
        "    return P\n",
        "\n",
        "def main():\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    cookies = get_cookies()\n",
        "\n",
        "    df = scrape_facebook(cookies)\n",
        "\n",
        "    if not df.empty:\n",
        "        filename = f\"{CONFIG['fanpage']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"資料已保存到 {filename}\")\n",
        "        print(f\"共爬取了 {len(df)} 條貼文\")\n",
        "    else:\n",
        "        print(\"沒有爬取到任何資料\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# 注意事項：\n",
        "# 1. 此腳本需要使用者提供 Facebook cookies，請確保安全使用且不要分享您的 cookies。\n",
        "# 2. 使用爬蟲可能違反 Facebook 的服務條款。請確保您有權限訪問和使用您爬取的資料。\n",
        "# 3. 設置適當的爬取間隔以避免被 Facebook 封鎖。\n",
        "# 4. 本腳本僅供教育和研究目的使用。在實際應用中，請遵守所有相關法律和平台政策。"
      ],
      "metadata": {
        "id": "f6lt-1i_cYpm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}